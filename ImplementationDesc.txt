Cambios con respecto a la implementación del paper original:

- La unidad de tiempo en la simulación es en segudos.
- Action space:
    - Continuo, pero se discretizo a nivel agente para ser utilizado con DDQN.
    - Limitado a 23 acciones que abarcan el rango [1000, 0] a [0, 1000].
- Reward:
    - Igual que en el paper original.
    - Se almacena el reward recolectado y se penaliza por cada estacion a la que no llego un tren con un valor extremadamente negativo (-10000)
    - El reward es comunicado al agente en el ultimo step de la simulación (Disperse reward).
- Escenario reducido a 2 trenes para comprobar viabilidad.